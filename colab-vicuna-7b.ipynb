{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX2181EsViJg",
        "outputId": "e1c8bb30-b14f-432a-a1dd-b9fcdc11204a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:9.3.0-1ubuntu2).\n",
            "g++ set to manually installed.\n",
            "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
            "gcc set to manually installed.\n",
            "make is already the newest version (4.2.1-1.2).\n",
            "make set to manually installed.\n",
            "cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n",
            "git is already the newest version (1:2.25.1-1ubuntu3.11).\n",
            "wget is already the newest version (1.20.3-1ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 2615, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 2615 (delta 3), reused 4 (delta 1), pack-reused 2599\u001b[K\n",
            "Receiving objects: 100% (2615/2615), 2.36 MiB | 9.48 MiB/s, done.\n",
            "Resolving deltas: 100% (1692/1692), done.\n",
            "--2023-05-08 21:41:09--  https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.9.86.125, 65.9.86.104, 65.9.86.43, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.9.86.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/bc/f7/bcf7007d6ddb945394a060a2ec7ea6f01b4687be53a6c2e36a5894036383887c/ec7772cf5b8faa9ee433cb1a8e1f6ec1c342ce7b831abcfb444bf07e63002234?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-vicuna-7b-1.1-q4_0.bin%3B+filename%3D%22ggml-vicuna-7b-1.1-q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1683841270&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2JjL2Y3L2JjZjcwMDdkNmRkYjk0NTM5NGEwNjBhMmVjN2VhNmYwMWI0Njg3YmU1M2E2YzJlMzZhNTg5NDAzNjM4Mzg4N2MvZWM3NzcyY2Y1YjhmYWE5ZWU0MzNjYjFhOGUxZjZlYzFjMzQyY2U3YjgzMWFiY2ZiNDQ0YmYwN2U2MzAwMjIzND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM4NDEyNzB9fX1dfQ__&Signature=rlv4SAUBv7mjo-I2VdcmtvG%7EvmIJnazpMFeLpdN--uOffgwkA5R5mpNjHPavW6N%7EH3jmVnSpEfM-1Ix6LqY0FsiETFWSqjXO%7EwewN6KXADLDKdemqLluEOx8AYlmpclaHLkkgmpI6-MFt7kootGxh%7EUPmig70QsqVvRnQZS%7EotDR346q1DbiXapDF889EBljaDq%7EXpKKHpMkD2z5RaGK-3QHHOBPNltnzyb80EzMVRLs-N%7E5a1G-p4yW7j2m4JK3r2xViSWiMgS7-qundl-sPAkABKIbKyvzIKEGz8-jaIsWNyUp5wrkEgC%7EmIOmpyizMgzgxx-yGwoUz7keMQTavQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-08 21:41:09--  https://cdn-lfs.huggingface.co/repos/bc/f7/bcf7007d6ddb945394a060a2ec7ea6f01b4687be53a6c2e36a5894036383887c/ec7772cf5b8faa9ee433cb1a8e1f6ec1c342ce7b831abcfb444bf07e63002234?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-vicuna-7b-1.1-q4_0.bin%3B+filename%3D%22ggml-vicuna-7b-1.1-q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1683841270&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2JjL2Y3L2JjZjcwMDdkNmRkYjk0NTM5NGEwNjBhMmVjN2VhNmYwMWI0Njg3YmU1M2E2YzJlMzZhNTg5NDAzNjM4Mzg4N2MvZWM3NzcyY2Y1YjhmYWE5ZWU0MzNjYjFhOGUxZjZlYzFjMzQyY2U3YjgzMWFiY2ZiNDQ0YmYwN2U2MzAwMjIzND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM4NDEyNzB9fX1dfQ__&Signature=rlv4SAUBv7mjo-I2VdcmtvG%7EvmIJnazpMFeLpdN--uOffgwkA5R5mpNjHPavW6N%7EH3jmVnSpEfM-1Ix6LqY0FsiETFWSqjXO%7EwewN6KXADLDKdemqLluEOx8AYlmpclaHLkkgmpI6-MFt7kootGxh%7EUPmig70QsqVvRnQZS%7EotDR346q1DbiXapDF889EBljaDq%7EXpKKHpMkD2z5RaGK-3QHHOBPNltnzyb80EzMVRLs-N%7E5a1G-p4yW7j2m4JK3r2xViSWiMgS7-qundl-sPAkABKIbKyvzIKEGz8-jaIsWNyUp5wrkEgC%7EmIOmpyizMgzgxx-yGwoUz7keMQTavQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.156.60.37, 108.156.60.112, 108.156.60.44, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.156.60.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4212859520 (3.9G) [application/octet-stream]\n",
            "Saving to: ‘/content/llama.cpp/ggml-vicuna-7b-1.1-q4_0.bin’\n",
            "\n",
            "ggml-vicuna-7b-1.1- 100%[===================>]   3.92G  37.0MB/s    in 1m 41s  \n",
            "\n",
            "2023-05-08 21:42:51 (39.7 MB/s) - ‘/content/llama.cpp/ggml-vicuna-7b-1.1-q4_0.bin’ saved [4212859520/4212859520]\n",
            "\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -c examples/common.cpp -o common.o\n",
            "\u001b[01m\u001b[Kllama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ksize_t llama_set_state_data(llama_context*, const uint8_t*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kllama.cpp:2615:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst uint8_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst unsigned char*\u001b[m\u001b[K’} to type ‘\u001b[01m\u001b[Kvoid*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K-Wcast-qual\u001b[m\u001b[K]\n",
            " 2615 |             kin3d->data = (void *) \u001b[01;35m\u001b[Kin\u001b[m\u001b[K;\n",
            "      |                                    \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kllama.cpp:2619:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst uint8_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst unsigned char*\u001b[m\u001b[K’} to type ‘\u001b[01m\u001b[Kvoid*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K-Wcast-qual\u001b[m\u001b[K]\n",
            " 2619 |             vin3d->data = (void *) \u001b[01;35m\u001b[Kin\u001b[m\u001b[K;\n",
            "      |                                    \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native pocs/vdot/vdot.cpp ggml.o -o vdot \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/main/main.cpp ggml.o llama.o common.o -o main \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/quantize/quantize.cpp ggml.o llama.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/quantize-stats/quantize-stats.cpp ggml.o llama.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y git cmake make gcc g++ wget\n",
        "\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Clonar el repositorio llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "os.chdir('llama.cpp')\n",
        "\n",
        "# Descargar el modelo desde Hugging Face\n",
        "!wget https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_0.bin -P /content/llama.cpp/\n",
        "# Compilar la biblioteca\n",
        "!make -j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de cómo hacer una consulta al Bot. (No almaceena el historial)\n",
        "!./main -m /content/llama.cpp/ggml-vicuna-7b-1.1-q4_0.bin -p \"User: Dime las ventajas de usar php. Assistant:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC-vRuypWXsS",
        "outputId": "85feb248-fe49-41d4-ae19-735ef574c67d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 523 (56551bc)\n",
            "main: seed  = 1683582270\n",
            "llama.cpp: loading model from /content/llama.cpp/ggml-vicuna-7b-1.1-q4_0.bin\n",
            "llama_model_load_internal: format     = ggjt v1 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =  68.20 KB\n",
            "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
            "llama_init_from_file: kv self size  =  256.00 MB\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            " User: Dime las ventajas de usar php. Assistant: PHP es una plataforma de programación interpretada que se utiliza para crear sitios web dinámicos. Aquí hay algunas ventajas de utilizar PHP en la creación de sitios web:\n",
            "\n",
            "1. Flexibilidad: PHP es un lenguaje muy flexible y versátil, lo que permite a los desarrolladores crear una gran variedad de aplicaciones y sitios web.\n",
            "2. Cambiar: PHP es un software de código abierto, lo que significa que es fácil de cambiar y actualizar. Esto es especialmente útil para la reparación de errores o la implementación de cambios en el sitio web.\n",
            "3. Personalización: PHP permite a los desarrolladores personalizar fácilmente el sitio web, ya sea mediante la modificación del código o mediante la introducción de plugins y complementos.\n",
            "4. Seguridad: PHP es un lenguaje muy seguro y proporciona una gran cantidad de herramientas para proteger el sitio web contra los ataques cibernéticos.\n",
            "5. Gran cantidad de recursos: Hay una gran cantidad de recursos disponibles para desarrolladores de PHP, incluyendo una amplia variedad de bibliotecas y frameworks, lo que facilita el desarrollo de aplicaciones y sitios web. [end of text]\n",
            "\n",
            "llama_print_timings:        load time =  9020.85 ms\n",
            "llama_print_timings:      sample time =   177.61 ms /   290 runs   (    0.61 ms per run)\n",
            "llama_print_timings: prompt eval time =  8847.17 ms /    16 tokens (  552.95 ms per token)\n",
            "llama_print_timings:        eval time = 215743.23 ms /   289 runs   (  746.52 ms per run)\n",
            "llama_print_timings:       total time = 225025.80 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de cómo ejecutar el modelo en modo chat\n",
        "!/content/llama.cpp/main -m /content/llama.cpp/ggml-vicuna-7b-1.1-q4_0.bin -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f /content/llama.cpp/prompts/chat-with-bob.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFOTqoWNagQ7",
        "outputId": "56370b94-0458-41b0-9bd1-33762510a42c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 523 (56551bc)\n",
            "main: seed  = 1683583108\n",
            "llama.cpp: loading model from /content/llama.cpp/ggml-vicuna-7b-1.1-q4_0.bin\n",
            "llama_model_load_internal: format     = ggjt v1 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =  68.20 KB\n",
            "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
            "llama_init_from_file: kv self size  =  256.00 MB\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "main: interactive mode on.\n",
            "Reverse prompt: 'User:'\n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.000000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0\n",
            "\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to LLaMa.\n",
            " - If you want to submit another line, end your input in '\\'.\n",
            "\n",
            "\u001b[33m Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n",
            "\n",
            "User: Hello, Bob.\n",
            "Bob: Hello. How may I help you today?\n",
            "User: Please tell me the largest city in Europe.\n",
            "Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.\n",
            "User:\u001b[0m\u001b[1m\u001b[32mHola\n",
            "\u001b[0m\u001b[1m\u001b[32m\u001b[0mBob: Hola. ¿Cómo estás?\n",
            "User:\u001b[1m\u001b[32mDime las ventajas de usar php\n",
            "\u001b[0mBob: Por supuesto. Las ventajas de usar PHP incluyen:\n",
            "\n",
            "* Integración con bases de datos: PHP es una herramienta de programación dinámica, lo que significa que puede interactuar con bases de datos y acceder a datos almacenados en ellas.\n",
            "* Flexibilidad: PHP es muy flexible y puede ser utilizado en una amplia variedad de entornos, desde servidores web a servidores de aplicaciones empresariales.\n",
            "* Gran comunidad: PHP tiene una gran comunidad de desarrolladores y es una herramienta muy popular, lo que significa que hay muchos recursos y documentación disponibles para ayudar a los desarrolladores a aprender y mejorar sus habilidades.\n",
            "* Integración con HTML y CSS: PHP es muy fácil de integrar con HTML y CSS, lo que significa que los desarrolladores pueden\n",
            "\n",
            "\n",
            "llama_print_timings:        load time = 51475.47 ms\n",
            "llama_print_timings:      sample time =     9.23 ms /   215 runs   (    0.04 ms per run)\n",
            "llama_print_timings: prompt eval time = 57922.36 ms /   112 tokens (  517.16 ms per token)\n",
            "llama_print_timings:        eval time = 161604.91 ms /   214 runs   (  755.16 ms per run)\n",
            "llama_print_timings:       total time = 228716.44 ms\n"
          ]
        }
      ]
    }
  ]
}

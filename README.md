# Vicuna.cpp
[![Deploy to Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Yahirovski/vicuna.cpp/blob/master/colab-vicuna-7b.ipynb?open_in_colab=1)

Run a fast ChatGPT-like model on your device, whether it be an ARM or x86_64 architecture. The screencast below is not sped up and demonstrates the model running smoothly on a VPS with ARM architecture and less than 4GB of RAM using the Vicuna 7B model.



## Consider using LLaMA.cpp instead

## One-click installation in Debian/Ubuntu (Works on Oracle Cloud's ARM Ampere VPS)
```
wget https://raw.githubusercontent.com/Yahirovski/Vicuna.cpp/master/ubuntu-install.sh; bash ubuntu-install.sh
```
